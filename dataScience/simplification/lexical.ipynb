{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lexical.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRgvjr5RdHaM"
      },
      "source": [
        "NER import statement  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qeo6OR-V5Esv"
      },
      "source": [
        "import spacy\r\n",
        "from spacy import displacy\r\n",
        "from collections import Counter\r\n",
        "import en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBIfZhKIdAR3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d89142-1e12-4ffa-a296-6d7dc256c18f"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1-IiIONa4oQ"
      },
      "source": [
        "Zip import statments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz8z2KRXITWT"
      },
      "source": [
        "pip install wordfreq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ARN_wjrL4s6"
      },
      "source": [
        "import re\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "import re\r\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \r\n",
        "from nltk.stem.lancaster import LancasterStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUqhRpt4bBsR"
      },
      "source": [
        "Bert import statement  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmqyeaZl-DtO"
      },
      "source": [
        "!pip install transformers\r\n",
        "!pip install torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5VjUIVE-IGK"
      },
      "source": [
        "import torch\r\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM\r\n",
        "\r\n",
        "bert_model = 'bert-large-uncased'\r\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model)\r\n",
        "model = BertForMaskedLM.from_pretrained(bert_model)\r\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTinGVYII9HN"
      },
      "source": [
        "#CWI using ZIP\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vzNu4bUI4uz"
      },
      "source": [
        "stop_words_ = set(stopwords.words('english'))\r\n",
        "\r\n",
        "# the word is turned to lower case and characters other than letters are removed\r\n",
        "def cleaner(word):\r\n",
        "  # Remove links\r\n",
        "  # word = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', \r\n",
        "  #               '', word, flags=re.MULTILINE)\r\n",
        "  # word = re.sub('[\\W]', ' ', word)\r\n",
        "  word = re.sub('[^a-zA-Z]', ' ', word)           \r\n",
        "  return word.lower().strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERpUfuVuI3qu"
      },
      "source": [
        "from wordfreq import zipf_frequency\r\n",
        "\r\n",
        "# Complex Word Identification\r\n",
        "def get_list_cwi_predictions(input_text):\r\n",
        "  list_cwi_predictions = []\r\n",
        "  list_of_words=word_tokenize(input_text)\r\n",
        "  for word in list_of_words:\r\n",
        "    word = cleaner(word)\r\n",
        "    if zipf_frequency(word, 'en') < 4:\r\n",
        "      list_cwi_predictions.append(True)\r\n",
        "    else:\r\n",
        "      list_cwi_predictions.append(False)\r\n",
        "  return list_cwi_predictions     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW8sF6ilJMeK"
      },
      "source": [
        "# BERT\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BuovKaT5MMV"
      },
      "source": [
        "# basic Named Entity Recognition code\r\n",
        "\r\n",
        "def NER_identifier(text):\r\n",
        "  entity_list=[]\r\n",
        "  nlp = en_core_web_sm.load()\r\n",
        "  doc = nlp(text)\r\n",
        "  for x in doc.ents:\r\n",
        "    entity_tokens = tokenizer.tokenize(x.text)\r\n",
        "    for word in entity_tokens:\r\n",
        "      entity_list.append(word)\r\n",
        "  return entity_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJJqsLfB-WXJ"
      },
      "source": [
        "# BERT model to predict candidates for identified complex words\r\n",
        "def get_bert_candidates(input_text, list_cwi_predictions, numb_predictions_displayed = 2):\r\n",
        "  \r\n",
        "  list_candidates_bert = []\r\n",
        "  names_enitites=NER_identifier(input_text)\r\n",
        "  for word,pred  in zip(input_text.split(), list_cwi_predictions):\r\n",
        "    \r\n",
        "    lowercase_word = word.lower()\r\n",
        "    \r\n",
        "    if pred and lowercase_word not in names_enitites :\r\n",
        "      replace_word_mask = input_text.replace(word, '[MASK]')\r\n",
        "      text = f'[CLS]{replace_word_mask} [SEP] {input_text} [SEP] '\r\n",
        "      tokenized_text = tokenizer.tokenize(text)\r\n",
        "      masked_index = [i for i, x in enumerate(tokenized_text) if x == '[MASK]'][0]\r\n",
        "      indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\r\n",
        "      segments_ids = [0]*len(tokenized_text)\r\n",
        "      tokens_tensor = torch.tensor([indexed_tokens])\r\n",
        "      segments_tensors = torch.tensor([segments_ids])\r\n",
        "      \r\n",
        "      # Predict all tokens\r\n",
        "      with torch.no_grad():\r\n",
        "          outputs = model(tokens_tensor, token_type_ids=segments_tensors)\r\n",
        "          predictions = outputs[0][0][masked_index]\r\n",
        "      predicted_ids = torch.argsort(predictions, descending=True)[:numb_predictions_displayed]\r\n",
        "      predicted_tokens = tokenizer.convert_ids_to_tokens(list(predicted_ids))\r\n",
        "      list_candidates_bert.append((word, predicted_tokens))\r\n",
        "\r\n",
        "  return list_candidates_bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhRUbkG7N_Xu"
      },
      "source": [
        "# Run the code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV7YCIT7V64I"
      },
      "source": [
        "def remove_punctuation(input):\r\n",
        "  x = re.sub(\"[^-9A-Za-z ]\", \"\" , input)\r\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOtjVX4lL_oQ"
      },
      "source": [
        "input_list = ['(5) Where an offence under subsection (1) is committed by a body corporate, any person who at the time of the commission of the offence was a director, general manager, secretary or other similar officer of the body corporate, or was purporting to act in any such capacity, shall be deemed to be guilty of that offence, unless he proves that the offence was committed without his consent or connivance and that he exercised all such diligence to prevent the commission of the offence as he ought to have exercised having regard to the nature of his functions in that capacity and to all the circumstances.'\r\n",
        " ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpY73fnV-fHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb724278-7364-4d26-a5d4-5bf1a696a14c"
      },
      "source": [
        "for input in input_list:\r\n",
        "  input_no_punc=remove_punctuation(input)\r\n",
        "  pred_list = get_list_cwi_predictions(input_no_punc)\r\n",
        "\r\n",
        "  ps = LancasterStemmer() \r\n",
        " \r\n",
        "  candidates = get_bert_candidates(input_no_punc, pred_list)\r\n",
        "  \r\n",
        "  for word,pred  in zip(input_no_punc.split(), pred_list):\r\n",
        "    if pred == True:\r\n",
        "      for candidate in candidates:\r\n",
        "        if candidate[0] == word:\r\n",
        "          replacement = \"\"\r\n",
        "          if zipf_frequency(candidate[1][0], 'en') > zipf_frequency(candidate[1][1], 'en'):\r\n",
        "            replacement = candidate[1][0]\r\n",
        "          else:\r\n",
        "            replacement = candidate[1][1]\r\n",
        "          \r\n",
        "          if ps.stem(replacement) == ps.stem(word) or (not replacement.isalpha()): \r\n",
        "            replacement = word\r\n",
        "          input = input.replace(word, replacement)\r\n",
        "  print('Output - ', input)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output -  (5) Where an offence under subsection (1) is committed by a body corporate, any person who at the time of the commission of the offence was a director, general manager, secretary or other similar officer of the body corporate, or was authorised to act in any such capacity, shall be deemed to be guilty of that offence, unless he proves that the offence was committed without his consent or knowledge and that he exercised all such power to prevent the commission of the offence as he ought to have exercised having regard to the nature of his functions in that capacity and to all the circumstances.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}