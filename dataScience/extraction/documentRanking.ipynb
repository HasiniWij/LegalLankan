{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "documentRanking.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "v41hxRCSjgfd"
      },
      "source": [
        "# imports necessary for query preprocessing\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzcVkJbLjyD-",
        "outputId": "c67d9dea-b514-4f23-dda0-70cf36ce5603"
      },
      "source": [
        "nltk.download('punkt')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igRYLl8ZdtX3"
      },
      "source": [
        "# imports necessary for document ranking\r\n",
        "from gensim import corpora, models, similarities\r\n",
        "import jieba\r\n",
        "import pickle\r\n",
        "import pandas as pd"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWiLayewjvgh",
        "outputId": "c4481db7-f804-4592-c1c1-79a4e11c702b"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNFnxe5tdPKT"
      },
      "source": [
        "# all necessary data related to the category that the query was classified into, is loaded here\r\n",
        "\r\n",
        "# loading the similarity matrix which stores document similarity data of all documents within the category \r\n",
        "matrix_path = \"/content/drive/MyDrive/SDGP code/Extraction/category/family/family_matrix.pickle\"\r\n",
        "\r\n",
        "with open(matrix_path, 'rb') as matrix:\r\n",
        "    category_matrix = pickle.load(matrix)\r\n",
        "\r\n",
        "# loading the dictionary of the category\r\n",
        "dictionary_path = \"/content/drive/MyDrive/SDGP code/Extraction/category/family/family_dic.pickle\"\r\n",
        "\r\n",
        "with open(dictionary_path, 'rb') as cat_dictionary:\r\n",
        "    dictionary = pickle.load(cat_dictionary)\r\n",
        "\r\n",
        "# loading the tfidf model of the category\r\n",
        "tfidf_path = \"/content/drive/MyDrive/SDGP code/Extraction/category/family/family_tfdif.pickle\"\r\n",
        "\r\n",
        "with open(tfidf_path, 'rb') as tfidf_model:\r\n",
        "    tfidf = pickle.load(tfidf_model)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgIZn4ODkEy7"
      },
      "source": [
        "# raw query entered by a user\r\n",
        "query = \"How can I get a divorce?\"\r\n",
        "\r\n",
        "query_lowered = query.lower()    # all letters are turned into lowercase\r\n",
        "\r\n",
        "punctuation_signs = list(\"?:!.,;\")   # punctuation signs are removed\r\n",
        "for punct_sign in punctuation_signs:\r\n",
        "    query_lowered = query_lowered.replace(punct_sign, '')\r\n",
        "\r\n",
        "query_lowered = query_lowered.replace(\"'s\", \"\")     # appostrophes are removed\r\n",
        "\r\n",
        "wordnet_lemmatizer = WordNetLemmatizer()       # all query words are lemmatized\r\n",
        "lemmatized_query_words_list = []       \r\n",
        "\r\n",
        "query_words = query_lowered.split(\" \")\r\n",
        "for word in query_words:\r\n",
        "        lemmatized_query_words_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\r\n",
        "\r\n",
        "stop_words = list(stopwords.words('english'))    # all stop words within the query are removed\r\n",
        "for word in lemmatized_query_words_list:\r\n",
        "  if word in stop_words:\r\n",
        "    lemmatized_query_words_list.remove(word)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZJKCPKLd6Rl"
      },
      "source": [
        "# all keywords after preprocessing, turned into a string of words\r\n",
        "keywords = ' '.join(map(str, lemmatized_query_words_list)) "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AkPKQISn-wK"
      },
      "source": [
        "# dictionary - convert search word to vector \r\n",
        "# jieba - cuts the kewords into segments \r\n",
        "# doc2bow - counts the number of occurrences of a word within the corpus\r\n",
        "kw_vector = dictionary.doc2bow(jieba.lcut(keywords))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mhkhc7fdvd2",
        "outputId": "bc300db9-9a5a-4423-b3ad-1038c69ce5d5"
      },
      "source": [
        "sim = category_matrix[tfidf[kw_vector]]  # checks the similarity of each document within the category\r\n",
        "for i in range(len(sim)):\r\n",
        "    if sim[i] > 0.00:\r\n",
        "      print('keyword similarity to text%d: %.2f' % (i + 1, sim[i]))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "keyword similarity to text38: 0.04\n",
            "keyword similarity to text39: 0.01\n",
            "keyword similarity to text42: 0.06\n",
            "keyword similarity to text134: 0.11\n",
            "keyword similarity to text143: 0.08\n",
            "keyword similarity to text145: 0.10\n",
            "keyword similarity to text153: 0.05\n",
            "keyword similarity to text156: 0.01\n",
            "keyword similarity to text160: 0.10\n",
            "keyword similarity to text166: 0.04\n",
            "keyword similarity to text171: 0.05\n",
            "keyword similarity to text177: 0.04\n",
            "keyword similarity to text181: 0.11\n",
            "keyword similarity to text194: 0.03\n",
            "keyword similarity to text202: 0.05\n",
            "keyword similarity to text208: 0.15\n",
            "keyword similarity to text209: 0.10\n",
            "keyword similarity to text210: 0.09\n",
            "keyword similarity to text211: 0.25\n",
            "keyword similarity to text212: 0.12\n",
            "keyword similarity to text213: 0.09\n",
            "keyword similarity to text233: 0.06\n",
            "keyword similarity to text275: 0.04\n",
            "keyword similarity to text279: 0.06\n",
            "keyword similarity to text317: 0.04\n",
            "keyword similarity to text349: 0.02\n",
            "keyword similarity to text475: 0.10\n",
            "keyword similarity to text476: 0.09\n",
            "keyword similarity to text482: 0.06\n",
            "keyword similarity to text491: 0.09\n",
            "keyword similarity to text493: 0.10\n",
            "keyword similarity to text494: 0.08\n",
            "keyword similarity to text496: 0.12\n",
            "keyword similarity to text505: 0.04\n",
            "keyword similarity to text508: 0.07\n",
            "keyword similarity to text509: 0.06\n",
            "keyword similarity to text511: 0.05\n",
            "keyword similarity to text514: 0.21\n",
            "keyword similarity to text516: 0.03\n",
            "keyword similarity to text601: 0.03\n",
            "keyword similarity to text609: 0.10\n",
            "keyword similarity to text621: 0.06\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}